# Hallucination(artificial intelligence)

When I am trying to solve the AI community issues on GitHub. I saw some issues about the hallucination but most of new user they are not familiar with this term. They beleive that hallucination is a bug of the API service. Moreover, according to Wekipedia. By 2023, analysts considered frequent hallucination to be a major problem in LLM tehchnology. And I have many of experience on hallucination on copilot. So I believe we need to know more about hallucination.

## What is hallucination?

A hallucination or artificial hallucination (also called confabulation or delusion) is a confident response by an AI that dies not seem to be justified by its training data (Wikipedia). Maybe you have seen with Github copilot, sometimes if you input the comment of the code, it will generate the tons of same content and repeat the same content. I think this is a direct example of hallucination.

But there is another example of hallucination. If you input the code of the comment, it will generate the code that is not related to the comment. For example, if you input the comment of the code, it will generate the code of the comment. This is a indirect example of hallucination.

In my opinion, these two examples are all hallucination. But the first one is more obvious than the second one. So, here is a potential issue. If we trust AI model and let it do all the tasks that we do not know the correct result. If the AI model has hallucination, we will trust the wrong result. This may cause the serious issue. So, we need to know more about hallucination and how to avoid it.


## What is the difference between human hallucination and AI hallucination?

According to the Wikipedia, one key difference between human hallucination and AI hallucination is that human hallucination is usually associated with false [percepts](https://en.wikipedia.org/wiki/Percept#Process_and_terminology), but an AI hallucination is associated with the category of unjustified responses or beliefs.


## How to avoid hallucination?

According to the Wikipedia, the hallucination phenomenon is still not completely understood. According to my experience on maintaining the neutral language model processing service, I think we need to make sure the prompt with the most valid format. And it also should include useful information.

Maybe you can imagine that you are trying to describe a question to a expert in the specific domain. You should be professional to describe the issue to him or her. This is also means that we need to know more than before, also we have AI technology to help us. We still to learn more than before.

## Reference

* [Hallucination (artificial intelligence)](https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence))
